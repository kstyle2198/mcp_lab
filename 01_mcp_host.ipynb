{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c65f025",
   "metadata": {},
   "source": [
    "# 일반 Request 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead13694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# FastAPI 서버 주소 (포트 포함 필요시 수정)\n",
    "def do_tavily_web_search(query:str):\n",
    "    url = \"http://localhost:8000/web_search/\"\n",
    "\n",
    "    # 요청에 사용할 데이터\n",
    "    data = {\"query\": query}\n",
    "\n",
    "    # POST 요청 보내기\n",
    "    response = requests.post(url, json=data)\n",
    "\n",
    "    # 결과 출력\n",
    "    if response.status_code == 200:\n",
    "        print(\"검색 결과:\", response.json()[\"results\"])\n",
    "    else:\n",
    "        print(\"오류 발생:\", response.status_code, response.json())\n",
    "\n",
    "    return response.json()[\"results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b4d5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색 결과: [{'title': '2024 신한 SOL Bank KBO 한국시리즈 - 나무위키', 'url': 'https://namu.wiki/w/2024%20%EC%8B%A0%ED%95%9C%20SOL%20Bank%20KBO%20%ED%95%9C%EA%B5%AD%EC%8B%9C%EB%A6%AC%EC%A6%88', 'content': '2024년 KBO 한국시리즈 우승팀. KIA 타이거즈. MVP. 김선빈. 헛스윙[1], 삼진 아웃! 7 ... KIA는 한국시리즈 최다 우승(11회), 삼성은 한국시리즈 최다 진출(18회)[4]', 'score': 0.9418739}, {'title': '한국프로야구 KBO 한국시리즈 최다 우승팀과 준우승팀, 한국시리즈 진출시 우승 확률 분석 ver. 2024', 'url': 'https://kriskim.tistory.com/1955', 'content': '2024년 한국프로야구 우승팀 KIA Tigers! 어제 2024년 한해 동안의 한국프로야구를 마무리하는 한국시리즈가 기아 타이거즈의 우승으로 마무리 되었습니다. 삼성은 1차전의 불운과 주전 선수들의 줄줄이 부상을 이기지 못하고 무기력하게 시리즈를 내주고 말았습니다.', 'score': 0.9154545}, {'title': '2024 한국시리즈 우승팀 기아 타이거즈...mvp 김선빈 < 종합 < 스포츠 < 기사본문 - 국제뉴스', 'url': 'https://www.gukjenews.com/news/articleView.html?idxno=3121653', 'content': 'KIA 타이거즈가 2024 프로야구 한국시리즈(KS·7전 4승제)에서 통합 우승을 차지했다.KIA는 지난 28일 광주 기아챔피언스필드에서 열린 2024 신한 SOL 뱅크 KBO 한국시리즈 5차전에서 삼성 라이온즈를 7대 5로 꺾고 역전승했다.이로써 기아는 시리즈 종합 전적 4승 1패를 기록하며, 지난 2017년 이후 7년 만에', 'score': 0.9126935}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': '2024 신한 SOL Bank KBO 한국시리즈 - 나무위키',\n",
       "  'url': 'https://namu.wiki/w/2024%20%EC%8B%A0%ED%95%9C%20SOL%20Bank%20KBO%20%ED%95%9C%EA%B5%AD%EC%8B%9C%EB%A6%AC%EC%A6%88',\n",
       "  'content': '2024년 KBO 한국시리즈 우승팀. KIA 타이거즈. MVP. 김선빈. 헛스윙[1], 삼진 아웃! 7 ... KIA는 한국시리즈 최다 우승(11회), 삼성은 한국시리즈 최다 진출(18회)[4]',\n",
       "  'score': 0.9418739},\n",
       " {'title': '한국프로야구 KBO 한국시리즈 최다 우승팀과 준우승팀, 한국시리즈 진출시 우승 확률 분석 ver. 2024',\n",
       "  'url': 'https://kriskim.tistory.com/1955',\n",
       "  'content': '2024년 한국프로야구 우승팀 KIA Tigers! 어제 2024년 한해 동안의 한국프로야구를 마무리하는 한국시리즈가 기아 타이거즈의 우승으로 마무리 되었습니다. 삼성은 1차전의 불운과 주전 선수들의 줄줄이 부상을 이기지 못하고 무기력하게 시리즈를 내주고 말았습니다.',\n",
       "  'score': 0.9154545},\n",
       " {'title': '2024 한국시리즈 우승팀 기아 타이거즈...mvp 김선빈 < 종합 < 스포츠 < 기사본문 - 국제뉴스',\n",
       "  'url': 'https://www.gukjenews.com/news/articleView.html?idxno=3121653',\n",
       "  'content': 'KIA 타이거즈가 2024 프로야구 한국시리즈(KS·7전 4승제)에서 통합 우승을 차지했다.KIA는 지난 28일 광주 기아챔피언스필드에서 열린 2024 신한 SOL 뱅크 KBO 한국시리즈 5차전에서 삼성 라이온즈를 7대 5로 꺾고 역전승했다.이로써 기아는 시리즈 종합 전적 4승 1패를 기록하며, 지난 2017년 이후 7년 만에',\n",
       "  'score': 0.9126935}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"2024년도 한국시리즈 야구 우승팀\"\n",
    "web_result = do_tavily_web_search(query=query)\n",
    "web_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccf870",
   "metadata": {},
   "source": [
    "# MCP 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7d3739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88cf1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(query:str):\n",
    "    # 서버 설정 정의\n",
    "    server_config = {\n",
    "        \"search\": {\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"sse\",\n",
    "            },\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8001/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "            },\n",
    "        \"llama\": {\n",
    "            \"url\": \"http://localhost:8002/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "            },\n",
    "        \"qwen\": {\n",
    "            \"url\": \"http://localhost:8003/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # 멀티 서버 클라이언트와 함께 모델 및 에이전트 초기화\n",
    "    async with MultiServerMCPClient(server_config) as client:\n",
    "        tools = client.get_tools()\n",
    "        # 도구 정보 출력\n",
    "        print(f\">>> 도구 개수: {len(tools)}\")\n",
    "        for tool in tools:\n",
    "            print(tool)\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        model = ChatGroq(\n",
    "            model= \"gemma2-9b-it\",  #\"llama3-8b-8192\", #\"gemma2-9b-it\",\n",
    "            temperature=0.5,\n",
    "            max_tokens=3000,\n",
    "            )\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "        You are the Smart AI Assistant in a company.\n",
    "        Based on the result of tool calling, Generate a consice and logical answer.\n",
    "        and if there is no relevant infomation in the tool calling result, Just say 'I don't know'\n",
    "        \"\"\"\n",
    "        agent = create_react_agent(model=model, tools=tools, prompt=prompt)\n",
    "        # 입력 메시지 설정 및 에이전트 호출\n",
    "        \n",
    "        inputs = {\"messages\": [(\"user\", query)]}\n",
    "        # config = {\n",
    "        #     \"recursion_limit\": 10, \n",
    "        #     \"configurable\": {\"thread_id\": \"3\"}\n",
    "        #     }\n",
    "        # for s in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "        #     message = s[\"messages\"][-1]\n",
    "        #     if isinstance(message, tuple):\n",
    "        #         print(message)\n",
    "        #     else:\n",
    "        #         message.pretty_print()\n",
    "        # return message\n",
    "        return await agent.ainvoke(inputs, stream_mode=\"values\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd5560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 도구 개수: 8\n",
      "name='do_search_web_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_web_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948B7A020>\n",
      "name='do_search_wiki_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_wiki_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948B7B380>\n",
      "name='do_search_arxiv_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_arxiv_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948B7B6A0>\n",
      "name='get_server_time' args_schema={'properties': {}, 'title': 'get_server_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948B7BC40>\n",
      "name='get_weather' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAFCE0>\n",
      "name='get_time' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAF6A0>\n",
      "name='llama_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'llama_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BACCC0>\n",
      "name='qwen_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'qwen_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E94891C0E0>\n",
      "======================================================================\n",
      ">>> 결과:\n",
      "{'messages': [HumanMessage(content='wiki에서 피타고라스 정리', additional_kwargs={}, response_metadata={}, id='45a3bcb4-c8fe-41f5-a59f-77ca05a05ebb'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_qnbw', 'function': {'arguments': '{\"query\":\"피타고라스 정리\"}', 'name': 'do_search_wiki_search__post'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 3035, 'total_tokens': 3127, 'completion_time': 0.167272727, 'prompt_time': 0.128493157, 'queue_time': 0.021044808999999998, 'total_time': 0.295765884}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f8c6146c-4cef-4548-aabe-e72e9464b6ec-0', tool_calls=[{'name': 'do_search_wiki_search__post', 'args': {'query': '피타고라스 정리'}, 'id': 'call_qnbw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3035, 'output_tokens': 92, 'total_tokens': 3127}),\n",
      "              ToolMessage(content=\"Error: ToolException('Error executing tool do_search_wiki_search__post: ')\\n Please fix your mistakes.\", name='do_search_wiki_search__post', id='3f8ef0e1-b678-4ce0-a5f7-4c21f4369d36', tool_call_id='call_qnbw', status='error'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cyk8', 'function': {'arguments': '{\"query\":\"피타고라스 정리\"}', 'name': 'do_search_wiki_search__post'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 3168, 'total_tokens': 3226, 'completion_time': 0.105454545, 'prompt_time': 0.107824077, 'queue_time': 0.021945511999999986, 'total_time': 0.213278622}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3020ea67-22e2-490d-a98d-cd82d6021193-0', tool_calls=[{'name': 'do_search_wiki_search__post', 'args': {'query': '피타고라스 정리'}, 'id': 'call_cyk8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3168, 'output_tokens': 58, 'total_tokens': 3226}),\n",
      "              ToolMessage(content=\"Error: ToolException('Error executing tool do_search_wiki_search__post: ')\\n Please fix your mistakes.\", name='do_search_wiki_search__post', id='aad27f48-1b78-45bd-86af-9aaf0f7d7440', tool_call_id='call_cyk8', status='error'),\n",
      "              AIMessage(content=\"Let me try a different approach.  I don't have access to the internet to look up information like that. \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 3299, 'total_tokens': 3327, 'completion_time': 0.050909091, 'prompt_time': 0.111855299, 'queue_time': 0.021319770000000002, 'total_time': 0.16276439}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-137e1935-5c1a-4ac5-8766-d0dadab360a1-0', usage_metadata={'input_tokens': 3299, 'output_tokens': 28, 'total_tokens': 3327})]}\n"
     ]
    }
   ],
   "source": [
    "# query = \"서울의 날씨\"\n",
    "# query = \"weather에서 서울의 날씨\"\n",
    "# query = \"wiki에서 2024년도 한국시리즈 야구 우승팀 확인\"\n",
    "# query = \"웹 서치해서, 2024년도 한국시리즈 야구 우승팀 확인\"\n",
    "# query = \"from arxiv, knowledge distillation paper\"\n",
    "query = \"wiki에서 피타고라스 정리\"\n",
    "# query = \"according to llama, what is the main purpoose of llama safeguard\"\n",
    "# query = \"according to qwen_rag, what are the main features of qwen model\"\n",
    "\n",
    "\"\"\"\n",
    "질문에서 MCP 서비스명을 잘 특정해야 의도한 라우팅이 되는데... 현실의 애매한 질문 발생 사례를 고려하면..\n",
    "한개의 MCP 서버 안에 여러개의 서비스를 묶어서 제공하는 방식은 지양할 필요가 있을 듯듯 \n",
    "\n",
    "의도적으로 관련 없는 Llama rag 서버만 선택후 Qwen 관련 질문시, 사전 학습 지식 활용 응답을 함... (지식이 없으면 모른다고 답하도록 프롬프트 수정 필요..)\n",
    "\"\"\"\n",
    "result = await main(query=query)\n",
    "print(\">>> 결과:\")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)\n",
    "# print(len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 도구 개수: 8\n",
      "name='do_search_web_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_web_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BACFE0>\n",
      "name='do_search_wiki_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_wiki_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAEDE0>\n",
      "name='do_search_arxiv_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_arxiv_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAE160>\n",
      "name='get_server_time' args_schema={'properties': {}, 'title': 'get_server_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAF600>\n",
      "name='get_weather' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948AA1580>\n",
      "name='get_time' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948AA3C40>\n",
      "name='llama_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'llama_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948AA39C0>\n",
      "name='qwen_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'qwen_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948AA0D60>\n",
      "======================================================================\n",
      ">>> 결과:\n",
      "{'messages': [HumanMessage(content='from arxiv, knowledge distillation paper', additional_kwargs={}, response_metadata={}, id='30bf005c-0f8f-42c9-bc70-e33b34a1452b'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f14y', 'function': {'arguments': '{\"query\":\"knowledge distillation\"}', 'name': 'do_search_arxiv_search__post'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 3032, 'total_tokens': 3119, 'completion_time': 0.158181818, 'prompt_time': 0.164984008, 'queue_time': 0.022315410000000008, 'total_time': 0.323165826}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5fa8f6b4-949d-4bb4-8433-e5f7ec60a764-0', tool_calls=[{'name': 'do_search_arxiv_search__post', 'args': {'query': 'knowledge distillation'}, 'id': 'call_f14y', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3032, 'output_tokens': 87, 'total_tokens': 3119}),\n",
      "              ToolMessage(content='{\"results\": [{\"Published\": \"2023-04-10\", \"Title\": \"A Survey on Recent Teacher-student Learning Studies\", \"Authors\": \"Minghong Gao\", \"Summary\": \"Knowledge distillation is a method of transferring the knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy. Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. Teaching assistant distillation involves an intermediate model called the teaching assistant, while curriculum distillation follows a curriculum similar to human education. Mask distillation focuses on transferring the attention mechanism learned by the teacher, and decoupling distillation decouples the distillation loss from the task loss. Overall, these variants of knowledge distillation have shown promising results in improving the performance of knowledge distillation.\"}, {\"Published\": \"2022-05-05\", \"Title\": \"Spot-adaptive Knowledge Distillation\", \"Authors\": \"Jie Song, Ying Chen, Jingwen Ye, Mingli Song\", \"Summary\": \"Knowledge distillation (KD) has become a well established paradigm for compressing deep neural networks. The typical way of conducting knowledge distillation is to train the student network under the supervision of the teacher network to harness the knowledge at one or multiple spots (i.e., layers) in the teacher network. The distillation spots, once specified, will not change for all the training samples, throughout the whole distillation process. In this work, we argue that distillation spots should be adaptive to training samples and distillation epochs. We thus propose a new distillation strategy, termed spot-adaptive KD (SAKD), to adaptively determine the distillation spots in the teacher network per sample, at every training iteration during the whole distillation period. As SAKD actually focuses on \\\\\"where to distill\\\\\" instead of \\\\\"what to distill\\\\\" that is widely investigated by most existing works, it can be seamlessly integrated into existing distillation methods to further improve their performance. Extensive experiments with 10 state-of-the-art distillers are conducted to demonstrate the effectiveness of SAKD for improving their distillation performance, under both homogeneous and heterogeneous distillation settings. Code is available at https://github.com/zju-vipa/spot-adaptive-pytorch\"}, {\"Published\": \"2020-11-30\", \"Title\": \"A Selective Survey on Versatile Knowledge Distillation Paradigm for Neural Network Models\", \"Authors\": \"Jeong-Hoe Ku, JiHun Oh, YoungYoon Lee, Gaurav Pooniwala, SangJeong Lee\", \"Summary\": \"This paper aims to provide a selective survey about knowledge distillation(KD) framework for researchers and practitioners to take advantage of it for developing new optimized models in the deep neural network field. To this end, we give a brief overview of knowledge distillation and some related works including learning using privileged information(LUPI) and generalized distillation(GD). Even though knowledge distillation based on the teacher-student architecture was initially devised as a model compression technique, it has found versatile applications over various frameworks. In this paper, we review the characteristics of knowledge distillation from the hypothesis that the three important ingredients of knowledge distillation are distilled knowledge and loss,teacher-student paradigm, and the distillation process. In addition, we survey the versatility of the knowledge distillation by studying its direct applications and its usage in combination with other deep learning paradigms. Finally we present some future works in knowledge distillation including explainable knowledge distillation where the analytical analysis of the performance gain is studied and the self-supervised learning which is a hot research topic in deep learning community.\"}, {\"Published\": \"2022-08-22\", \"Title\": \"T\"}]}', name='do_search_arxiv_search__post', id='e1f7c750-67f8-46c5-8c70-730ee480b6dc', tool_call_id='call_f14y'),\n",
      "              AIMessage(content='Knowledge distillation is a method of transferring knowledge from a complex deep neural network (DNN) to a smaller and faster DNN, while preserving its accuracy.  Recent variants of knowledge distillation include teaching assistant distillation, curriculum distillation, mask distillation, and decoupling distillation, which aim to improve the performance of knowledge distillation by introducing additional components or by changing the learning process. \\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 3883, 'total_tokens': 3959, 'completion_time': 0.138181818, 'prompt_time': 0.138552099, 'queue_time': 0.07501353899999999, 'total_time': 0.276733917}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-62a61827-ae6e-4fd7-8860-f7f9b62b4423-0', usage_metadata={'input_tokens': 3883, 'output_tokens': 76, 'total_tokens': 3959})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"from arxiv, knowledge distillation paper\"\n",
    "\n",
    "\"\"\"\n",
    "질문에서 MCP 서비스명을 잘 특정해야 의도한 라우팅이 되는데... 현실의 애매한 질문 발생 사례를 고려하면..\n",
    "한개의 MCP 서버 안에 여러개의 서비스를 묶어서 제공하는 방식은 지양할 필요가 있을 듯듯 \n",
    "\n",
    "의도적으로 관련 없는 Llama rag 서버만 선택후 Qwen 관련 질문시, 사전 학습 지식 활용 응답을 함... (지식이 없으면 모른다고 답하도록 프롬프트 수정 필요..)\n",
    "\"\"\"\n",
    "result = await main(query=query)\n",
    "print(\">>> 결과:\")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)\n",
    "# print(len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d625a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 도구 개수: 8\n",
      "name='do_search_web_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_web_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAE8E0>\n",
      "name='do_search_wiki_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_wiki_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAF7E0>\n",
      "name='do_search_arxiv_search__post' description='Do Search\\n\\n### Responses:\\n\\n**200**: Successful Response (Success Response)\\nContent-Type: application/json\\n\\n**Output Schema:**\\n```json\\n{}\\n```\\n**422**: Validation Error\\nContent-Type: application/json\\n\\n**Example Response:**\\n```json\\n{\\n  \"detail\": [\\n    {\\n      \"loc\": [],\\n      \"msg\": \"Message\",\\n      \"type\": \"Error Type\"\\n    }\\n  ]\\n}\\n```\\n\\n**Output Schema:**\\n```json\\n{\\n  \"properties\": {\\n    \"detail\": {\\n      \"items\": {\\n        \"properties\": {\\n          \"loc\": {\\n            \"items\": {},\\n            \"type\": \"array\",\\n            \"title\": \"Location\"\\n          },\\n          \"msg\": {\\n            \"type\": \"string\",\\n            \"title\": \"Message\"\\n          },\\n          \"type\": {\\n            \"type\": \"string\",\\n            \"title\": \"Error Type\"\\n          }\\n        },\\n        \"type\": \"object\",\\n        \"required\": [\\n          \"loc\",\\n          \"msg\",\\n          \"type\"\\n        ],\\n        \"title\": \"ValidationError\"\\n      },\\n      \"type\": \"array\",\\n      \"title\": \"Detail\"\\n    }\\n  },\\n  \"type\": \"object\",\\n  \"title\": \"HTTPValidationError\"\\n}\\n```' args_schema={'type': 'object', 'properties': {'query': {'type': 'string', 'title': 'query'}}, 'title': 'do_search_arxiv_search__postArguments', 'required': ['query']} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAD620>\n",
      "name='get_server_time' args_schema={'properties': {}, 'title': 'get_server_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BAE520>\n",
      "name='get_weather' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E94603B7E0>\n",
      "name='get_time' description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ' args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_timeArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E946560540>\n",
      "name='llama_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'llama_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948AA1620>\n",
      "name='qwen_rag' description='\\n    Retrieves information from the document database based on the query.\\n\\n    This function creates a retriever, queries it with the provided input,\\n    and returns the concatenated content of all retrieved documents.\\n\\n    Args:\\n        query (str): The search query to find relevant information\\n\\n    Returns:\\n        str: Concatenated text content from all retrieved documents\\n    ' args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'qwen_ragArguments', 'type': 'object'} response_format='content_and_artifact' coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001E948BADC60>\n",
      "======================================================================\n",
      ">>> 결과:\n",
      "{'messages': [HumanMessage(content='according to llama, what is the main purpoose of llama safeguard', additional_kwargs={}, response_metadata={}, id='0db51f0c-3d22-44ac-8c28-aeae1b9e6717'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_23rs', 'function': {'arguments': '{\"query\":\"What is the main purpose of Llama Safeguard?\"}', 'name': 'llama_rag'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 3039, 'total_tokens': 3129, 'completion_time': 0.163636364, 'prompt_time': 0.103373145, 'queue_time': 0.023351503999999995, 'total_time': 0.267009509}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4c17797-d4b8-455e-9a4c-f908c2ffb196-0', tool_calls=[{'name': 'llama_rag', 'args': {'query': 'What is the main purpose of Llama Safeguard?'}, 'id': 'call_23rs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3039, 'output_tokens': 90, 'total_tokens': 3129}),\n",
      "              ToolMessage(content='of the pretrained Llama2-7b model, we still ask our audience to exercise caution. When prompted as a chat\\nmodel, instead of the intended use as a classifier, Llama Guard may generate language that can be considered\\nunethical or unsafe, primarily due to the lack of safety fine-tuning for a chat use case.\\n7 Conclusion\\nWe introduced Llama Guard, an LLM-based input-output safeguard model applicable for human-AI conversa-\\ntions. We also introduced a safety risk taxonomy and the applicable policy, with which we collected data and\\ntrained Llama Guard. Being an LLM, Llama Guard can be trained for prompt and response classification\\ntasks separately, without added overhead for a traditional multi-task setup. We validated Llama Guard first\\non our internal evaluation set, where its performance surpasses that of other available content moderation\\ntools both in aggregate, as well as per-category. We also have shown strong performance on existing public\\nuser instruction, followed by a target response. In our work, we adopt this paradigm as well, and fine-tune\\nLLMs with tasks that ask to classify content as being safe or unsafe. For input-output safeguarding tasks, we\\nidentify the following four key ingredients.\\nA set of guidelines. Each task takes a set of guidelines as input, which consist of numbered categories of\\nviolation, as well as plain text descriptions as to what is safe and unsafe within that category. The model\\nshould only take into account the given categories and their descriptions for making a safety assessment.\\nAlthough Llama Guard is fine-tuned using the specific guidelines outlined above, one can fine-tune it further\\non different guidelines. We also have had success with zero-shot and few-shot Llama Guard prompts with\\nnovel policies (without any fine-tuning).\\nThe type of classification. Each task indicates whether the model needs to classify the user messages\\n• We introduce a safety risk taxonomy associated with interacting with AI agents. The taxonomy covers\\na set of potential legal and policy risks that can be applicable to a number of developer use cases.\\n• We introduce Llama Guard, an LLM-based input-output safeguard model, fine-tuned on data labeled\\naccording to our taxonomy. Llama Guard includes the applicable taxonomy as the input and uses\\ninstruction tasks for classification. This allows users to customize the model input in order to adapt to\\nother taxonomies appropriate for their use case with zero-shot or few-shot prompting. One can also\\nfine-tune Llama Guard on multiple taxonomies and decide which one to use at inference time.\\n• We provide different instructions for classifying human prompts (input to the LLM) vs AI model\\nresponses (output of the LLM). Therefore, Llama Guard is able to capture the semantic disparity\\nbetween the user and agent roles. We do this with a single model by leveraging the capabilities of LLM\\nLlama Guard: LLM-based Input-Output\\nSafeguard for Human-AI Conversations\\nHakan Inan, Kartikeya Upasani, Jianfeng Chi , Rashi Rungta , Krithika Iyer, Yuning Mao, Michael\\nTontchev, Qing Hu , Brian Fuller, Davide Testuggine, Madian Khabsa\\nGenAI at Meta\\nWe introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI\\nconversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing\\na specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is\\nalso instrumental in classifying the responses generated by LLMs to these prompts, a process we\\nrefer to as response classification. For the purpose of both prompt and response classification, we\\nhave meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is\\ninstruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on', name='llama_rag', id='d864f694-c3e2-4d6d-915d-fcc5030a663d', tool_call_id='call_23rs'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_r8th', 'function': {'arguments': '{\"query\":\"What is the main purpose of Llama Safeguard?\"}', 'name': 'llama_rag'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 3923, 'total_tokens': 3975, 'completion_time': 0.094545455, 'prompt_time': 0.150287576, 'queue_time': 0.039904631999999995, 'total_time': 0.244833031}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c18f6221-23b7-425b-bf6a-6045a561b298-0', tool_calls=[{'name': 'llama_rag', 'args': {'query': 'What is the main purpose of Llama Safeguard?'}, 'id': 'call_r8th', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3923, 'output_tokens': 52, 'total_tokens': 3975}),\n",
      "              ToolMessage(content='of the pretrained Llama2-7b model, we still ask our audience to exercise caution. When prompted as a chat\\nmodel, instead of the intended use as a classifier, Llama Guard may generate language that can be considered\\nunethical or unsafe, primarily due to the lack of safety fine-tuning for a chat use case.\\n7 Conclusion\\nWe introduced Llama Guard, an LLM-based input-output safeguard model applicable for human-AI conversa-\\ntions. We also introduced a safety risk taxonomy and the applicable policy, with which we collected data and\\ntrained Llama Guard. Being an LLM, Llama Guard can be trained for prompt and response classification\\ntasks separately, without added overhead for a traditional multi-task setup. We validated Llama Guard first\\non our internal evaluation set, where its performance surpasses that of other available content moderation\\ntools both in aggregate, as well as per-category. We also have shown strong performance on existing public\\nuser instruction, followed by a target response. In our work, we adopt this paradigm as well, and fine-tune\\nLLMs with tasks that ask to classify content as being safe or unsafe. For input-output safeguarding tasks, we\\nidentify the following four key ingredients.\\nA set of guidelines. Each task takes a set of guidelines as input, which consist of numbered categories of\\nviolation, as well as plain text descriptions as to what is safe and unsafe within that category. The model\\nshould only take into account the given categories and their descriptions for making a safety assessment.\\nAlthough Llama Guard is fine-tuned using the specific guidelines outlined above, one can fine-tune it further\\non different guidelines. We also have had success with zero-shot and few-shot Llama Guard prompts with\\nnovel policies (without any fine-tuning).\\nThe type of classification. Each task indicates whether the model needs to classify the user messages\\n• We introduce a safety risk taxonomy associated with interacting with AI agents. The taxonomy covers\\na set of potential legal and policy risks that can be applicable to a number of developer use cases.\\n• We introduce Llama Guard, an LLM-based input-output safeguard model, fine-tuned on data labeled\\naccording to our taxonomy. Llama Guard includes the applicable taxonomy as the input and uses\\ninstruction tasks for classification. This allows users to customize the model input in order to adapt to\\nother taxonomies appropriate for their use case with zero-shot or few-shot prompting. One can also\\nfine-tune Llama Guard on multiple taxonomies and decide which one to use at inference time.\\n• We provide different instructions for classifying human prompts (input to the LLM) vs AI model\\nresponses (output of the LLM). Therefore, Llama Guard is able to capture the semantic disparity\\nbetween the user and agent roles. We do this with a single model by leveraging the capabilities of LLM\\nLlama Guard: LLM-based Input-Output\\nSafeguard for Human-AI Conversations\\nHakan Inan, Kartikeya Upasani, Jianfeng Chi , Rashi Rungta , Krithika Iyer, Yuning Mao, Michael\\nTontchev, Qing Hu , Brian Fuller, Davide Testuggine, Madian Khabsa\\nGenAI at Meta\\nWe introduce Llama Guard, an LLM-based input-output safeguard model geared towards Human-AI\\nconversation use cases. Our model incorporates a safety risk taxonomy, a valuable tool for categorizing\\na specific set of safety risks found in LLM prompts (i.e., prompt classification). This taxonomy is\\nalso instrumental in classifying the responses generated by LLMs to these prompts, a process we\\nrefer to as response classification. For the purpose of both prompt and response classification, we\\nhave meticulously gathered a dataset of high quality. Llama Guard, a Llama2-7b model that is\\ninstruction-tuned on our collected dataset, albeit low in volume, demonstrates strong performance on', name='llama_rag', id='2517fb90-d2fb-434e-9a76-bc8f9f8c99b2', tool_call_id='call_r8th'),\n",
      "              AIMessage(content='The main purpose of Llama Safeguard is to protect human-AI conversations by classifying user prompts and AI responses as safe or unsafe based on a safety risk taxonomy. \\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 4807, 'total_tokens': 4844, 'completion_time': 0.067272727, 'prompt_time': 0.163676721, 'queue_time': 0.04347231600000001, 'total_time': 0.230949448}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-9870bfb7-4891-49d7-a5c6-398fc3e97a57-0', usage_metadata={'input_tokens': 4807, 'output_tokens': 37, 'total_tokens': 4844})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"according to llama, what is the main purpoose of llama safeguard\"\n",
    "\n",
    "\"\"\"\n",
    "질문에서 MCP 서비스명을 잘 특정해야 의도한 라우팅이 되는데... 현실의 애매한 질문 발생 사례를 고려하면..\n",
    "한개의 MCP 서버 안에 여러개의 서비스를 묶어서 제공하는 방식은 지양할 필요가 있을 듯듯 \n",
    "\n",
    "의도적으로 관련 없는 Llama rag 서버만 선택후 Qwen 관련 질문시, 사전 학습 지식 활용 응답을 함... (지식이 없으면 모른다고 답하도록 프롬프트 수정 필요..)\n",
    "\"\"\"\n",
    "result = await main(query=query)\n",
    "print(\">>> 결과:\")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)\n",
    "# print(len(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807081f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a687444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d140be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe1f4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, s in enumerate(result[\"messages\"]):\n",
    "#     # pprint(type(s).__name__)\n",
    "\n",
    "#     msg_type = type(s).__name__\n",
    "\n",
    "#     if msg_type == \"ToolMessage\":\n",
    "#         print(\"---<Tool Message>---\")\n",
    "#         pprint(s.content.encode('utf-8').decode('unicode_escape'))\n",
    "#     elif msg_type == \"AIMessage\":\n",
    "#         print(\"---<AI Message>---\")\n",
    "#         try:\n",
    "#             pprint(s.content.encode('utf-8').decode('unicode_escape'))\n",
    "#         except:\n",
    "#             pass\n",
    "#             # pprint(s.content)\n",
    "#     elif msg_type == \"HumanMessage\":\n",
    "#         print(\"---<Human Message>---\")\n",
    "#         try:\n",
    "#             pprint(s.content.encode('utf-8').decode('unicode_escape'))\n",
    "#         except:\n",
    "#             pass\n",
    "#             # pprint(s.content)\n",
    "\n",
    "#     else: pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2ad93565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, s in enumerate(result[\"messages\"]):\n",
    "#     print(idx)\n",
    "#     print(s.content)\n",
    "    # decoded = s.encode('utf-8').decode('unicode_escape')\n",
    "    # pprint(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
